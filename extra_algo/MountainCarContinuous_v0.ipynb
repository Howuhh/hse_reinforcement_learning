{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "MountainCarContinuous-v0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9LGpvxOujt1"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KL1hFkD3JJc"
      },
      "source": [
        "# MountainCarContinuous-v0 \n",
        "\n",
        "Во втором задании нужно решить MountainCarContinuous-v0, где в отличии от обычного MountainCar используются вещественные действия (то есть одно) - отрицательное значение заставляет машинку ехать назад, а положительное вперед. Интересна и функция реварда - агенту дается 100 очков за заезд на горку за вычетом суммы квадратов всех действий. Из этого следует, что если агент не заедет на гору достаточно быстро, он выучится стоять на месте и получать 0.0 награды.\n",
        "\n",
        "Решением считается 90 награды в среднем за 100 запусков.\n",
        "\n",
        "## DDPG & TD3\n",
        "\n",
        "Среды с вещественными действиями можно решать несколькими способами. Самым простым вариантом будет дискретизировать пространство действий и использовать обычный DQN как в прошлом задании. Тем не менее, есть алгоритмы специально созданные для работы с подобными средами, например DDPG. DDPG в идее очень похоже на DQN и является просто расширением на вещественные действия (action's), где мы используем отдельную сеть Actor, которая учится находить действие, максимизирующее Q функцию. \n",
        "\n",
        "У DDPG есть известные недостатки, которые попытались исправить в также очень известном алгоритме Twin Delayed DDPG или TD3. Основное отличие - использование двух сетей для оценок Q функции, чтобы поправить overestimation bias, а слово Delayed говорит о том, что Actor обновляется только раз в $N$ обновлений Critic'a. \n",
        "\n",
        "Алгоритм для меня новый (совсем) и я, признаюсь, кажется не заставил его работать до конца. Он легко решает задачу, но только при некоторых гиперпараметрах и даже если решает, то после получения необходимого реварда быстро расходится и скатывается в -99 реварда вместо нужных 90. В целом я не придумал как его стабилизровать, поэтому для задания просто останавливаю обучение при достижении среднего реварда 90 за 100 запусков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcPU8ZGnujt8"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "        \n",
        "    def add(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "        \n",
        "    def sample(self, size):\n",
        "        batch = random.sample(self.buffer, size)\n",
        "        return list(zip(*batch))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWTQV5Mpujt9"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size)\n",
        "        )        \n",
        "    \n",
        "    def forward(self, state):\n",
        "        out = self.actor(state)\n",
        "        return torch.tanh(out)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_size + action_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        ) \n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], dim=1)\n",
        "        return self.critic(state_action).view(-1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FDqrYbiujt9"
      },
      "source": [
        "class TD3:\n",
        "    def __init__(self, state_size, action_size, gamma=0.99, tau=0.005, \n",
        "                       actor_lr=1e-3, critic_lr=1e-3, target_noise=0.1):\n",
        "        self.actor = Actor(state_size, action_size).to(device)\n",
        "        self.target_actor = deepcopy(self.actor).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        \n",
        "        self.critic1 = Critic(state_size, action_size).to(device)\n",
        "        self.target_critic1 = deepcopy(self.critic1).to(device)\n",
        "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=critic_lr)\n",
        "        \n",
        "        self.critic2 = Critic(state_size, action_size).to(device)\n",
        "        self.target_critic2 = deepcopy(self.critic2).to(device)\n",
        "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=critic_lr)\n",
        "        \n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.target_noise = target_noise\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(state, device=device, dtype=torch.float32)\n",
        "            action = self.actor(state).cpu().numpy()\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def soft_update(self, target_net, source_net):\n",
        "        for target_param, source_param, in zip(target_net.parameters(), source_net.parameters()):\n",
        "            update = (1 - self.tau) * target_param.data + self.tau * source_param.data\n",
        "            target_param.data.copy_(update)\n",
        "\n",
        "    def actor_loss(self, state):\n",
        "        loss = -self.critic1(state, self.actor(state)).mean()\n",
        "        return loss\n",
        "    \n",
        "    def critic_loss(self, state, action, reward, next_state, done):\n",
        "        with torch.no_grad():\n",
        "            next_action = self.target_actor(next_state)\n",
        "            next_action = torch.clip(next_action + self.target_noise * torch.randn_like(next_action), -1.0, 1.0)\n",
        "            \n",
        "            Q_next = torch.minimum(\n",
        "                self.target_critic1(next_state, next_action), \n",
        "                self.target_critic2(next_state, next_action)\n",
        "            )\n",
        "            Q_target = reward + self.gamma * (1 - done) * Q_next\n",
        "        \n",
        "        Q1 = self.critic1(state, action)\n",
        "        Q2 = self.critic2(state, action)\n",
        "        \n",
        "        critic1_loss = F.mse_loss(Q1, Q_target)\n",
        "        critic2_loss = F.mse_loss(Q2, Q_target)\n",
        "        \n",
        "        return critic1_loss + critic2_loss\n",
        "        \n",
        "    def update(self, batch, update_actor=False):\n",
        "        state, action, reward, new_state, done = batch\n",
        "        \n",
        "        state = torch.tensor(state, device=device, dtype=torch.float32)\n",
        "        action = torch.tensor(action, device=device, dtype=torch.long)\n",
        "        reward = torch.tensor(reward, device=device, dtype=torch.float32)\n",
        "        next_state = torch.tensor(new_state, device=device, dtype=torch.float32)\n",
        "        done = torch.tensor(done, device=device, dtype=torch.float32)\n",
        "        \n",
        "        critic_losses = self.critic_loss(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Critic1 & Critic2 update\n",
        "        self.critic1_optimizer.zero_grad()\n",
        "        self.critic2_optimizer.zero_grad()\n",
        "        critic_losses.backward()\n",
        "        self.critic1_optimizer.step()\n",
        "        self.critic2_optimizer.step()\n",
        "        \n",
        "        if update_actor:\n",
        "            # Actor update\n",
        "            actor_loss = self.actor_loss(state)\n",
        "            \n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "            \n",
        "            #  Target networks soft update\n",
        "            with torch.no_grad():\n",
        "                self.soft_update(self.target_actor, self.actor)\n",
        "                self.soft_update(self.target_critic1, self.critic1)\n",
        "                self.soft_update(self.target_critic2, self.critic2)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IadHo_W3ujt-"
      },
      "source": [
        "def rollout(env, model):\n",
        "    total_reward = 0.0\n",
        "    done, state = False, env.reset()\n",
        "    \n",
        "    while not done:\n",
        "        action = model.act(state)\n",
        "        \n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "def evaluate_model(model, episodes=10):\n",
        "    env = gym.make(\"MountainCarContinuous-v0\")\n",
        "    env.seed(22) \n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        rewards.append(rollout(env, model))\n",
        "\n",
        "    return np.mean(rewards), np.std(rewards)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be_1IB6uujt_"
      },
      "source": [
        "def train(model, goal_reward, timesteps=500_000, start_train=10_000, buffer_size=100_000, batch_size=512, test_every=5000,\n",
        "          policy_delay=2, max_action_noise=0.1, min_action_noise=0.1):\n",
        "\n",
        "    env = gym.make(\"MountainCarContinuous-v0\")\n",
        "    \n",
        "    env.seed(22)\n",
        "    random.seed(22)\n",
        "    np.random.seed(22)\n",
        "    torch.manual_seed(22)\n",
        "  \n",
        "    reward_means, reward_std = [], []\n",
        "    \n",
        "    buffer = ReplayBuffer(size=buffer_size)\n",
        "    done, state = False, env.reset()\n",
        "\n",
        "    shaping_scale = 300\n",
        "\n",
        "    for step in range(timesteps):\n",
        "        if done:\n",
        "            done, state = False, env.reset()\n",
        "\n",
        "        action_noise = max_action_noise - (max_action_noise - min_action_noise) * step / timesteps\n",
        "\n",
        "        action = model.act(state)\n",
        "        action = np.clip(action + action_noise * np.random.randn(len(action)), -1.0, 1.0)\n",
        "            \n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        reward = reward + shaping_scale * (model.gamma * abs(new_state[1]) - abs(state[1])) \n",
        "        buffer.add((state, action, reward, new_state, int(new_state[0] >= 0.5)))\n",
        "        \n",
        "        state = new_state\n",
        "        \n",
        "        if step >= start_train:\n",
        "            shaping_scale = 0.0\n",
        "\n",
        "            update_actor = True if step % policy_delay == 0 else False\n",
        "            model.update(buffer.sample(size=batch_size), update_actor=update_actor)\n",
        "            \n",
        "            if step % test_every == 0 or step == timesteps - 1:\n",
        "                mean, std = evaluate_model(model, episodes=10)\n",
        "\n",
        "                reward_means.append(mean)\n",
        "                reward_std.append(std)\n",
        "                \n",
        "                print(f\"Step: {step}, Mean reward: {mean:.2f}, std: {std:.2f}, action_noise: {action_noise:.4f}\")\n",
        "\n",
        "                if mean >= goal_reward and evaluate_model(model, episodes=100)[0] >= goal_reward:\n",
        "                    print(\"SOLVED\")\n",
        "                    break\n",
        "\n",
        "    return np.array(reward_means), np.array(reward_std)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvJIBIYL_zeG"
      },
      "source": [
        "# Обучение Агента"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIK-17cFujuA",
        "outputId": "c87b8c0d-5cd6-4bfd-ab52-79aff1bf4e9e"
      },
      "source": [
        "td3 = TD3(2, 1, tau=0.001, gamma=0.99, actor_lr=1e-3, critic_lr=1e-3, target_noise=0.0)\n",
        "\n",
        "means, stds = train(td3, goal_reward=90, timesteps=50_000, start_train=4096, batch_size=256, max_action_noise=0.5, min_action_noise=0.1, \n",
        "                    policy_delay=2, buffer_size=4096, test_every=1000) "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 5000, Mean reward: -3.89, std: 0.28, action_noise: 0.4600\n",
            "Step: 6000, Mean reward: 94.31, std: 1.34, action_noise: 0.4520\n",
            "SOLVED\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}