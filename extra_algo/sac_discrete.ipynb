{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBwTmXa5-RHr",
    "outputId": "1977fe6d-4e6d-4f4b-ef63-a757058a7b6e"
   },
   "outputs": [],
   "source": [
    "# !pip install gym[box2d]\n",
    "# !pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pwb5JFhh-T7K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cymZF61g-SCG"
   },
   "outputs": [],
   "source": [
    "def set_seed(env, seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        \n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, size):\n",
    "        batch = random.sample(self.buffer, size)\n",
    "        return list(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UmxFYjxD-aBP"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, eval_mode=False, return_probs=False):\n",
    "        logits = self.actor(state)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        policy_dist = Categorical(probs=probs)\n",
    "\n",
    "        if eval_mode:\n",
    "            action = torch.argmax(probs, dim=-1)\n",
    "        else:\n",
    "            action = policy_dist.sample()\n",
    "\n",
    "        if return_probs:\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            return action, probs, log_probs\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):        \n",
    "        return self.critic(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "LYhXGdkvEAS9"
   },
   "outputs": [],
   "source": [
    "class SoftActorCritic:\n",
    "    def __init__(self, state_size, action_size, hidden_size, target_entropy_scale=0.5, gamma=0.99, tau=0.005, init_alpha=None, actor_lr=1e-4, critic_lr=1e-4, alpha_lr=1e-4):\n",
    "        self.actor = Actor(state_size, action_size, hidden_size).to(DEVICE)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        \n",
    "        self.critic1 = Critic(state_size, action_size, hidden_size).to(DEVICE)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=critic_lr)\n",
    "        self.target_critic1 = deepcopy(self.critic1)\n",
    "        \n",
    "        self.critic2 = Critic(state_size, action_size, hidden_size).to(DEVICE)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=critic_lr)\n",
    "        self.target_critic2 = deepcopy(self.critic2)\n",
    "\n",
    "        for p in itertools.chain(self.target_critic1.parameters(), self.target_critic2.parameters()):\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.init_alpha = 0.0 if init_alpha is None else np.log(init_alpha)\n",
    "        self.target_entropy = -np.log((1.0 / action_size)) * target_entropy_scale # * 0.98\n",
    "  \n",
    "        self.log_alpha = torch.tensor([self.init_alpha], dtype=torch.float32, device=DEVICE, requires_grad=True)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "\n",
    "    def _soft_update(self, target, source):\n",
    "        for tp, sp in zip(target.parameters(), source.parameters()):\n",
    "            tp.data.copy_((1 - self.tau) * tp.data + self.tau * sp.data)\n",
    "            \n",
    "    def _actor_loss(self, state):\n",
    "        action, action_probs, action_log_probs = self.actor(state, return_probs=True)\n",
    "\n",
    "        Q_target = torch.min(\n",
    "            self.critic1(state), \n",
    "            self.critic2(state)\n",
    "        )\n",
    "        # True expectation over all actions + sample estimate for expectation over states\n",
    "        loss = ((self.alpha.detach() * action_log_probs - Q_target.detach()) * action_probs).sum(dim=1).mean()\n",
    "        \n",
    "        assert action_log_probs.shape == Q_target.shape == action_probs.shape\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _critic_loss(self, state, action, reward, next_state, done):\n",
    "        with torch.no_grad():\n",
    "            next_action, next_action_probs, next_action_log_probs = self.actor(next_state, return_probs=True)\n",
    "            \n",
    "            Q_min = torch.min(\n",
    "                self.target_critic1(next_state),\n",
    "                self.target_critic2(next_state)\n",
    "            )\n",
    "            # True expectation over actions to estimate V(s')\n",
    "            Q_next = (next_action_probs * (Q_min - self.alpha * next_action_log_probs)).sum(dim=1)\n",
    "            Q_target = reward + self.gamma * (1 - done) * Q_next\n",
    "\n",
    "            assert Q_next.shape == reward.shape\n",
    "            assert next_action_probs.shape == Q_min.shape == next_action_log_probs.shape\n",
    "        \n",
    "        # NOTE: gather need (batch_size, 1) action shape, not (batch_size,)\n",
    "        Q1 = self.critic1(state).gather(1, action.reshape(-1, 1).long()).view(-1)\n",
    "        Q2 = self.critic2(state).gather(1, action.reshape(-1, 1).long()).view(-1)\n",
    "        \n",
    "        loss = F.mse_loss(Q1, Q_target) + F.mse_loss(Q2, Q_target)\n",
    "\n",
    "        assert Q1.shape == Q_target.shape and Q2.shape == Q_target.shape\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def _alpha_loss(self, state):\n",
    "        with torch.no_grad():\n",
    "            action, action_probs, action_log_probs = self.actor(state, return_probs=True)\n",
    "            # https://github.com/yining043/SAC-discrete/issues/2#event-3685116634\n",
    "            action_log_probs_exp = (action_log_probs * action_probs).sum(dim=1)\n",
    "\n",
    "        loss = (-self.log_alpha * (action_log_probs_exp + self.target_entropy)).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update(self, batch):\n",
    "        state, action, reward, next_state, done = batch\n",
    "        \n",
    "        state = torch.tensor(state, device=DEVICE, dtype=torch.float32)\n",
    "        action = torch.tensor(action, device=DEVICE, dtype=torch.float32)\n",
    "        reward = torch.tensor(reward, device=DEVICE, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, device=DEVICE, dtype=torch.float32)\n",
    "        done = torch.tensor(done, device=DEVICE, dtype=torch.float32)\n",
    "        \n",
    "        # Critic1 & Critic2 update\n",
    "        critic_losses = self._critic_loss(state, action, reward, next_state, done)\n",
    "\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic_losses.backward()\n",
    "        self.critic1_optimizer.step()\n",
    "        self.critic2_optimizer.step()\n",
    "        \n",
    "        # Actor update    \n",
    "        actor_loss = self._actor_loss(state)\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Alpha update\n",
    "        alpha_loss = self._alpha_loss(state)\n",
    "\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        #  Target networks soft update\n",
    "        with torch.no_grad():\n",
    "            self._soft_update(self.target_critic1, self.critic1)\n",
    "            self._soft_update(self.target_critic2, self.critic2)\n",
    "\n",
    "    def act(self, state, eval_mode=False):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=DEVICE, dtype=torch.float32)\n",
    "            action = self.actor(state, eval_mode=eval_mode).cpu().numpy().item()\n",
    "        return action\n",
    "    \n",
    "    def save(self, name):\n",
    "        torch.save(self.actor.state_dict(), f\"{name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vIxMfdXvTHf5"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env_name, agent, seed, episodes=5):\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    set_seed(env, seed)\n",
    "    \n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        total_reward = 0.\n",
    "        \n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(agent.act(state, eval_mode=True))\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "    \n",
    "    return np.mean(returns), np.std(returns)\n",
    "\n",
    "\n",
    "def train(env_name, model, seed=0, timesteps=500_000, start_steps=10_000, start_train=1000, \n",
    "          buffer_size=100_000, batch_size=512, test_episodes=10, test_every=5000, update_every=10):\n",
    "    print(\"Training on: \", DEVICE)\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    set_seed(env, seed)\n",
    "    \n",
    "    buffer = ReplayBuffer(size=buffer_size)\n",
    "    best_reward = -np.inf\n",
    "    \n",
    "    means, stds = [], []\n",
    "    \n",
    "    done, state = False, env.reset()\n",
    "    \n",
    "    for t in range(timesteps):\n",
    "        if done:\n",
    "            done, state = False, env.reset()\n",
    "    \n",
    "        if t > start_train:\n",
    "            action = model.act(state)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "        buffer.add((state, action, reward, next_state, done))\n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "        if t > start_train:\n",
    "            if t % update_every == 0:\n",
    "                for _ in range(update_every):\n",
    "                    batch = buffer.sample(batch_size)\n",
    "                    model.update(batch)\n",
    "            \n",
    "            if t % test_every == 0 or t == timesteps - 1:\n",
    "                mean, std = evaluate_policy(env_name, model, seed=seed, episodes=test_episodes)\n",
    "                print(f\"Step: {t + 1}, Reward mean: {mean}, Reward std: {std}, Alpha: {model.alpha.detach().cpu().item()}\")\n",
    "                \n",
    "                if mean > best_reward:\n",
    "                    best_reward = mean\n",
    "                    model.save(f\"best_agent\")\n",
    "                \n",
    "                model.save(f\"last_agent\")\n",
    "    \n",
    "                means.append(mean)\n",
    "                stds.append(std)\n",
    "    \n",
    "    return np.array(means), np.array(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1PbuV8oTpFA",
    "outputId": "601ae38a-e77d-492d-a404-3893763cdfeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  cpu\n",
      "Step: 15001, Reward mean: -133.17716541093182, Reward std: 112.7897131906716, Alpha: 0.9701305627822876\n",
      "Step: 20001, Reward mean: -306.56213472737164, Reward std: 101.9606333482044, Alpha: 0.9440081715583801\n",
      "Step: 25001, Reward mean: -243.99490862548282, Reward std: 80.72378346974962, Alpha: 0.9334538578987122\n",
      "Step: 30001, Reward mean: -320.6924113084025, Reward std: 76.14900610805385, Alpha: 0.9224791526794434\n",
      "Step: 35001, Reward mean: -249.19914899674717, Reward std: 104.39374501747429, Alpha: 0.8974756598472595\n",
      "Step: 40001, Reward mean: -33.65693273371791, Reward std: 33.649856075483804, Alpha: 0.8644418120384216\n",
      "Step: 45001, Reward mean: -145.39037736904862, Reward std: 195.11835006181573, Alpha: 0.8276879787445068\n",
      "Step: 50001, Reward mean: -6.984071233585493, Reward std: 98.56343890939584, Alpha: 0.7903392314910889\n",
      "Step: 55001, Reward mean: -38.3534994829545, Reward std: 34.71027722539631, Alpha: 0.7543269991874695\n",
      "Step: 60001, Reward mean: -18.367844848200754, Reward std: 25.927177350648368, Alpha: 0.7196934223175049\n",
      "Step: 65001, Reward mean: -13.41316426050759, Reward std: 29.082160160608222, Alpha: 0.6864781975746155\n",
      "Step: 70001, Reward mean: -12.828830843276766, Reward std: 24.697737291527396, Alpha: 0.6545822024345398\n",
      "Step: 75001, Reward mean: -24.393287387156374, Reward std: 24.64675562007005, Alpha: 0.6246464252471924\n",
      "Step: 80001, Reward mean: -18.45191439710975, Reward std: 27.34486984548176, Alpha: 0.5959912538528442\n",
      "Step: 85001, Reward mean: -20.80011341889567, Reward std: 23.559253670553932, Alpha: 0.5692778825759888\n",
      "Step: 90001, Reward mean: -27.578084034759193, Reward std: 37.93540898470367, Alpha: 0.5445742607116699\n",
      "Step: 95001, Reward mean: 23.18346598783192, Reward std: 66.36619225193215, Alpha: 0.5228145122528076\n",
      "Step: 100001, Reward mean: -14.994768956242618, Reward std: 24.921998769784675, Alpha: 0.5044366121292114\n",
      "Step: 105001, Reward mean: -12.790389212530389, Reward std: 67.02683455609083, Alpha: 0.48908647894859314\n",
      "Step: 110001, Reward mean: -7.28999486989301, Reward std: 75.01944897570364, Alpha: 0.4752125144004822\n",
      "Step: 115001, Reward mean: -52.25470689249913, Reward std: 34.79549431388213, Alpha: 0.46397924423217773\n",
      "Step: 120001, Reward mean: -12.609214940992434, Reward std: 96.46095752429673, Alpha: 0.45478227734565735\n",
      "Step: 125001, Reward mean: 89.58977492575391, Reward std: 133.35013645232354, Alpha: 0.44697046279907227\n",
      "Step: 130001, Reward mean: 54.22908310519695, Reward std: 139.10999901584097, Alpha: 0.43748903274536133\n",
      "Step: 135001, Reward mean: 37.79068434124109, Reward std: 139.35628866081734, Alpha: 0.42578125\n",
      "Step: 140001, Reward mean: 22.166519608528258, Reward std: 130.53594269649952, Alpha: 0.4163845181465149\n",
      "Step: 145001, Reward mean: -57.535369548833884, Reward std: 109.84498261552228, Alpha: 0.40763378143310547\n",
      "Step: 150001, Reward mean: 24.449868672212375, Reward std: 128.73499839497205, Alpha: 0.3977797031402588\n",
      "Step: 155001, Reward mean: 50.176523305033506, Reward std: 146.36825635083207, Alpha: 0.38783425092697144\n",
      "Step: 160001, Reward mean: 95.43572473414564, Reward std: 139.77352303627066, Alpha: 0.37885403633117676\n",
      "Step: 165001, Reward mean: 51.52361588260603, Reward std: 123.43114079061199, Alpha: 0.3717760443687439\n",
      "Step: 170001, Reward mean: 85.50290694112368, Reward std: 124.768799325947, Alpha: 0.3653298020362854\n",
      "Step: 175001, Reward mean: 122.12043384175134, Reward std: 114.75083199170494, Alpha: 0.3595163822174072\n",
      "Step: 180001, Reward mean: 89.63151974254497, Reward std: 114.8676402314199, Alpha: 0.35397255420684814\n",
      "Step: 185001, Reward mean: 115.27922378860049, Reward std: 115.20468111226742, Alpha: 0.34814363718032837\n",
      "Step: 190001, Reward mean: 149.45811185781326, Reward std: 102.18375746458422, Alpha: 0.34337118268013\n",
      "Step: 195001, Reward mean: 130.5586633396556, Reward std: 126.92085601952486, Alpha: 0.3386826515197754\n",
      "Step: 200001, Reward mean: 138.28309102229622, Reward std: 103.57011216933384, Alpha: 0.333973228931427\n",
      "Step: 205001, Reward mean: 161.6699693678103, Reward std: 101.0565841502104, Alpha: 0.32894182205200195\n",
      "Step: 210001, Reward mean: 167.56937805883277, Reward std: 102.8969423703142, Alpha: 0.32087668776512146\n",
      "Step: 215001, Reward mean: 10.978938035826562, Reward std: 209.76600052455942, Alpha: 0.3138316869735718\n",
      "Step: 220001, Reward mean: 104.92245522770459, Reward std: 107.84066803028276, Alpha: 0.3058420717716217\n",
      "Step: 225001, Reward mean: 203.8267491541522, Reward std: 73.59669835939167, Alpha: 0.29758015275001526\n",
      "Step: 230001, Reward mean: 134.26624788721304, Reward std: 126.44854745315908, Alpha: 0.2898178994655609\n",
      "Step: 235001, Reward mean: 192.57022716727016, Reward std: 119.45852058592432, Alpha: 0.28201931715011597\n",
      "Step: 240001, Reward mean: 237.69362929525428, Reward std: 26.75219974555886, Alpha: 0.275490403175354\n",
      "Step: 245001, Reward mean: 236.25817541749296, Reward std: 18.52926403126894, Alpha: 0.27131426334381104\n",
      "Step: 250001, Reward mean: 222.13565775208218, Reward std: 46.78690416671637, Alpha: 0.2673831284046173\n",
      "Step: 255001, Reward mean: 209.65707831186123, Reward std: 67.28905850011378, Alpha: 0.2659931182861328\n",
      "Step: 260001, Reward mean: 226.41280369179873, Reward std: 27.653602280242122, Alpha: 0.2634727358818054\n",
      "Step: 265001, Reward mean: 242.25603952211176, Reward std: 16.143154118979066, Alpha: 0.2602960765361786\n",
      "Step: 270001, Reward mean: 162.53626169645278, Reward std: 124.97353420732438, Alpha: 0.25615736842155457\n",
      "Step: 275001, Reward mean: 247.41811861542675, Reward std: 22.322078159004665, Alpha: 0.2528171241283417\n",
      "Step: 280001, Reward mean: 230.51587433516934, Reward std: 52.48467079906994, Alpha: 0.24796773493289948\n",
      "Step: 285001, Reward mean: 227.90358142152172, Reward std: 29.267753766823375, Alpha: 0.24701723456382751\n",
      "Step: 290001, Reward mean: 162.25336939982503, Reward std: 179.21176633068131, Alpha: 0.24563813209533691\n",
      "Step: 295001, Reward mean: 244.74336006812723, Reward std: 25.118787464065328, Alpha: 0.24241043627262115\n",
      "Step: 300001, Reward mean: 217.80144200274225, Reward std: 52.11895144233978, Alpha: 0.2413180023431778\n",
      "Step: 305001, Reward mean: 183.61660721644756, Reward std: 135.5060503263539, Alpha: 0.24062226712703705\n",
      "Step: 310001, Reward mean: 238.47423256876624, Reward std: 13.651651468297294, Alpha: 0.23905369639396667\n",
      "Step: 315001, Reward mean: 234.2191728948016, Reward std: 30.72664769679707, Alpha: 0.23788505792617798\n",
      "Step: 320001, Reward mean: 248.91643013922004, Reward std: 25.48359619191537, Alpha: 0.23805983364582062\n",
      "Step: 325001, Reward mean: 195.9342979026858, Reward std: 124.96449822521888, Alpha: 0.23608233034610748\n",
      "Step: 330001, Reward mean: 226.2375025436395, Reward std: 43.644546026920814, Alpha: 0.23910219967365265\n",
      "Step: 335001, Reward mean: 239.48290337400036, Reward std: 8.737083232818835, Alpha: 0.24119316041469574\n",
      "Step: 340001, Reward mean: 235.53668540751156, Reward std: 13.251823885296172, Alpha: 0.2408810555934906\n",
      "Step: 345001, Reward mean: 197.9547738125289, Reward std: 85.6612586004652, Alpha: 0.23681719601154327\n",
      "Step: 350001, Reward mean: 241.69648860982198, Reward std: 26.541139219355845, Alpha: 0.23428525030612946\n",
      "Step: 355001, Reward mean: 248.50017565181253, Reward std: 16.438684304306538, Alpha: 0.2325706034898758\n",
      "Step: 360001, Reward mean: 240.03535549040413, Reward std: 19.928648414379634, Alpha: 0.23412807285785675\n",
      "Step: 365001, Reward mean: 215.92631208866683, Reward std: 67.52162200516617, Alpha: 0.2344132661819458\n",
      "Step: 370001, Reward mean: 247.26703581245405, Reward std: 15.466707693072836, Alpha: 0.23631243407726288\n",
      "Step: 375001, Reward mean: 239.06429388046345, Reward std: 42.87771007615779, Alpha: 0.23845013976097107\n",
      "Step: 380001, Reward mean: 247.46090179460265, Reward std: 21.995108661665228, Alpha: 0.2406463325023651\n",
      "Step: 385001, Reward mean: 256.1667724738447, Reward std: 13.972961135410294, Alpha: 0.2424127459526062\n",
      "Step: 390001, Reward mean: 256.6378799332884, Reward std: 18.913922203458828, Alpha: 0.24341319501399994\n",
      "Step: 395001, Reward mean: 258.1033924458803, Reward std: 17.470928707871135, Alpha: 0.24404999613761902\n",
      "Step: 400001, Reward mean: 248.2661941380846, Reward std: 35.502865306300514, Alpha: 0.2436329871416092\n",
      "Step: 405001, Reward mean: 263.87523292607176, Reward std: 26.159737566359038, Alpha: 0.24270525574684143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 410001, Reward mean: 263.76928066978905, Reward std: 20.09837264035266, Alpha: 0.24392205476760864\n",
      "Step: 415001, Reward mean: 258.87617431820644, Reward std: 18.829408812404335, Alpha: 0.24353379011154175\n",
      "Step: 420001, Reward mean: 254.00089449362773, Reward std: 13.806342092321147, Alpha: 0.24307413399219513\n",
      "Step: 425001, Reward mean: 271.4616880709311, Reward std: 20.109551818209056, Alpha: 0.24234504997730255\n",
      "Step: 430001, Reward mean: 260.36916988678433, Reward std: 24.153386429750697, Alpha: 0.24122145771980286\n",
      "Step: 435001, Reward mean: 248.5031923294765, Reward std: 18.226278315623222, Alpha: 0.24139465391635895\n",
      "Step: 440001, Reward mean: 252.07126946737156, Reward std: 14.671098940396414, Alpha: 0.23885147273540497\n",
      "Step: 445001, Reward mean: 271.05518137527264, Reward std: 19.43150894518037, Alpha: 0.23836444318294525\n",
      "Step: 450001, Reward mean: 265.1309925113035, Reward std: 17.420395767192723, Alpha: 0.23740257322788239\n",
      "Step: 455001, Reward mean: 261.80858486030485, Reward std: 19.010392401142724, Alpha: 0.2373354285955429\n",
      "Step: 460001, Reward mean: 257.3246117685884, Reward std: 12.201419702889815, Alpha: 0.23713283240795135\n",
      "Step: 465001, Reward mean: 238.60786902333416, Reward std: 36.04287799129513, Alpha: 0.23678064346313477\n",
      "Step: 470001, Reward mean: 261.4511656390058, Reward std: 41.22426108518253, Alpha: 0.23522061109542847\n",
      "Step: 475001, Reward mean: 259.6588118564543, Reward std: 21.544835445788067, Alpha: 0.23616303503513336\n",
      "Step: 480001, Reward mean: 273.13693659254614, Reward std: 19.886802412240876, Alpha: 0.23640643060207367\n",
      "Step: 485001, Reward mean: 264.14481922746415, Reward std: 15.520484405808013, Alpha: 0.23596151173114777\n",
      "Step: 490001, Reward mean: 260.5769493141952, Reward std: 25.665484901781518, Alpha: 0.2345038801431656\n",
      "Step: 495001, Reward mean: 275.7016063007689, Reward std: 17.236155283470456, Alpha: 0.23456138372421265\n",
      "Step: 500000, Reward mean: 270.84743561623793, Reward std: 20.01085203844453, Alpha: 0.2372826486825943\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"agent\": {\n",
    "        \"state_size\": 8,\n",
    "        \"action_size\": 4,\n",
    "        \"hidden_size\": 256,\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.001,\n",
    "        \"target_entropy_scale\": 0.5, # 0.5\n",
    "        \"actor_lr\": 2e-4,\n",
    "        \"critic_lr\": 5e-4,\n",
    "        \"alpha_lr\": 1e-5 # 1e-5\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"seed\": 0,\n",
    "        \"timesteps\": 500_000,\n",
    "        \"start_train\": 10_000,\n",
    "        \"buffer_size\": 200_000, # better than int(1e6)\n",
    "        \"batch_size\": 128,\n",
    "        \"test_episodes\": 10,\n",
    "        \"test_every\": 5_000,\n",
    "        \"update_every\": 16 # 16\n",
    "    }\n",
    "}\n",
    "\n",
    "model = SoftActorCritic(**config[\"agent\"])\n",
    "mean, std = train(\"LunarLander-v2\", model, **config[\"trainer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  cpu\n",
      "Step: 15001, Reward mean: -171.79445337753612, Reward std: 21.144142598497574, Alpha: 0.9613027572631836\n",
      "Step: 20001, Reward mean: -136.55867166895368, Reward std: 22.70925298460754, Alpha: 0.9203715920448303\n",
      "Step: 25001, Reward mean: -118.87268968611443, Reward std: 22.955041044022288, Alpha: 0.8770214319229126\n",
      "Step: 30001, Reward mean: -116.2477305722422, Reward std: 22.043151304277806, Alpha: 0.8369368314743042\n",
      "Step: 35001, Reward mean: -74.91471377896605, Reward std: 23.09738939391834, Alpha: 0.8026368021965027\n",
      "Step: 40001, Reward mean: -23.8266696096837, Reward std: 24.081517606353543, Alpha: 0.7704234719276428\n",
      "Step: 45001, Reward mean: -0.9064542077316127, Reward std: 21.95486304383988, Alpha: 0.7393550872802734\n",
      "Step: 50001, Reward mean: -2.056823901816847, Reward std: 22.640738533302873, Alpha: 0.710578978061676\n",
      "Step: 55001, Reward mean: -11.75039547137502, Reward std: 22.638271968221492, Alpha: 0.6840417385101318\n",
      "Step: 60001, Reward mean: 44.59825678978723, Reward std: 81.56612727548546, Alpha: 0.6600213050842285\n",
      "Step: 65001, Reward mean: 184.4842100705003, Reward std: 50.26130457034566, Alpha: 0.636070966720581\n",
      "Step: 70001, Reward mean: -15.724935805710055, Reward std: 39.221056664348424, Alpha: 0.613028347492218\n",
      "Step: 75001, Reward mean: 144.62020721247237, Reward std: 94.85751063539115, Alpha: 0.5884865522384644\n",
      "Step: 80001, Reward mean: 15.896465767395933, Reward std: 53.133025795192474, Alpha: 0.5636677145957947\n",
      "Step: 85001, Reward mean: 199.51280777984124, Reward std: 35.453994519439, Alpha: 0.5396137833595276\n",
      "Step: 90001, Reward mean: 71.6402656955099, Reward std: 90.31918994683922, Alpha: 0.5166553258895874\n",
      "Step: 95001, Reward mean: 208.60960291039765, Reward std: 22.04898715197527, Alpha: 0.4948655366897583\n",
      "Step: 100001, Reward mean: 194.09170615792436, Reward std: 47.84506852272959, Alpha: 0.4746597409248352\n",
      "Step: 105001, Reward mean: 180.47385125164777, Reward std: 37.80069110418894, Alpha: 0.4559704065322876\n",
      "Step: 110001, Reward mean: 88.6628286156857, Reward std: 77.56167391019325, Alpha: 0.4392530620098114\n",
      "Step: 115001, Reward mean: 116.4116528588476, Reward std: 70.97399852910141, Alpha: 0.4255222678184509\n",
      "Step: 120001, Reward mean: 169.65590902332946, Reward std: 56.170945256493866, Alpha: 0.4134960472583771\n",
      "Step: 125001, Reward mean: 155.9748798941604, Reward std: 54.18312312891218, Alpha: 0.404136061668396\n",
      "Step: 130001, Reward mean: 130.1101940926423, Reward std: 51.7825515172099, Alpha: 0.3969104588031769\n",
      "Step: 135001, Reward mean: 175.04772307331194, Reward std: 71.88021357757754, Alpha: 0.3932639956474304\n",
      "Step: 140001, Reward mean: 189.43950547646148, Reward std: 48.221921397268304, Alpha: 0.3906933069229126\n",
      "Step: 145001, Reward mean: 134.89743989171316, Reward std: 74.75750391769316, Alpha: 0.3875862956047058\n",
      "Step: 150001, Reward mean: 176.23924818015928, Reward std: 67.58814073451764, Alpha: 0.3862709105014801\n",
      "Step: 155001, Reward mean: 212.80135082744437, Reward std: 36.1592188591817, Alpha: 0.384024977684021\n",
      "Step: 160001, Reward mean: 195.1359089153565, Reward std: 38.029784609445834, Alpha: 0.38211655616760254\n",
      "Step: 165001, Reward mean: 184.46373057881758, Reward std: 26.834635733044845, Alpha: 0.38039571046829224\n",
      "Step: 170001, Reward mean: 187.82738119193, Reward std: 44.64662280833286, Alpha: 0.3769523501396179\n",
      "Step: 175001, Reward mean: 195.7888271403277, Reward std: 15.430222226673894, Alpha: 0.37332290410995483\n",
      "Step: 180001, Reward mean: 185.39619954897788, Reward std: 54.80750632460062, Alpha: 0.3693901300430298\n",
      "Step: 185001, Reward mean: 190.14508411032728, Reward std: 15.141630547890104, Alpha: 0.36581483483314514\n",
      "Step: 190001, Reward mean: 179.78434622882259, Reward std: 15.042131473829732, Alpha: 0.3622792363166809\n",
      "Step: 195001, Reward mean: 173.29747337403995, Reward std: 39.998837760816016, Alpha: 0.3591037392616272\n",
      "Step: 200001, Reward mean: 131.37343574967923, Reward std: 66.9866826743516, Alpha: 0.35595202445983887\n",
      "Step: 205001, Reward mean: 123.01936027483369, Reward std: 73.67962852760466, Alpha: 0.3487526774406433\n",
      "Step: 210001, Reward mean: 156.03081509717694, Reward std: 122.74497687038857, Alpha: 0.3365602493286133\n",
      "Step: 215001, Reward mean: 184.78347139491922, Reward std: 20.86782154296795, Alpha: 0.32554876804351807\n",
      "Step: 220001, Reward mean: 158.14422068023498, Reward std: 63.616282100908116, Alpha: 0.313642680644989\n",
      "Step: 225001, Reward mean: 186.20333952781846, Reward std: 14.593303281030012, Alpha: 0.30263400077819824\n",
      "Step: 230001, Reward mean: 123.59744138028468, Reward std: 102.98714540607244, Alpha: 0.29051414132118225\n",
      "Step: 235001, Reward mean: 81.60156874698993, Reward std: 105.66474535011743, Alpha: 0.27901071310043335\n",
      "Step: 240001, Reward mean: 68.59300201865668, Reward std: 106.769056268092, Alpha: 0.26827043294906616\n",
      "Step: 245001, Reward mean: -25.06036903536211, Reward std: 48.89960551997409, Alpha: 0.25897693634033203\n",
      "Step: 250001, Reward mean: -40.83434657876009, Reward std: 28.173310432253214, Alpha: 0.24969260394573212\n",
      "Step: 255001, Reward mean: -14.640226271757044, Reward std: 62.996902306954006, Alpha: 0.24157938361167908\n",
      "Step: 260001, Reward mean: -0.08753615074646462, Reward std: 67.09051663571503, Alpha: 0.2341155707836151\n",
      "Step: 265001, Reward mean: 19.918502776029264, Reward std: 89.39448494234402, Alpha: 0.22665497660636902\n",
      "Step: 270001, Reward mean: 72.06967642666578, Reward std: 81.48815200792467, Alpha: 0.22073231637477875\n",
      "Step: 275001, Reward mean: 126.98347532214522, Reward std: 93.98639011564306, Alpha: 0.21446040272712708\n",
      "Step: 280001, Reward mean: 104.39055115426599, Reward std: 109.60397041060642, Alpha: 0.21037976443767548\n",
      "Step: 285001, Reward mean: 103.1701723725973, Reward std: 83.57197827368917, Alpha: 0.20631283521652222\n",
      "Step: 290001, Reward mean: 145.5116326952252, Reward std: 85.30747147274964, Alpha: 0.2016279697418213\n",
      "Step: 295001, Reward mean: 113.74798029690275, Reward std: 92.48813099050865, Alpha: 0.1986367404460907\n",
      "Step: 300001, Reward mean: 129.06434690113466, Reward std: 91.4212208828988, Alpha: 0.1973271369934082\n",
      "Step: 305001, Reward mean: 196.7878628007108, Reward std: 30.496482214555172, Alpha: 0.19708536565303802\n",
      "Step: 310001, Reward mean: 29.587745704386286, Reward std: 128.5118776398818, Alpha: 0.19799862802028656\n",
      "Step: 315001, Reward mean: 100.61776024923347, Reward std: 127.30626849517716, Alpha: 0.19641397893428802\n",
      "Step: 320001, Reward mean: 85.65515382320123, Reward std: 114.693111404389, Alpha: 0.19503873586654663\n",
      "Step: 325001, Reward mean: 177.57643411202804, Reward std: 88.68730465811997, Alpha: 0.1918535828590393\n",
      "Step: 330001, Reward mean: 127.67683210165401, Reward std: 105.81563474013015, Alpha: 0.18977978825569153\n",
      "Step: 335001, Reward mean: 189.75154227500397, Reward std: 72.76665299968576, Alpha: 0.18786701560020447\n",
      "Step: 340001, Reward mean: 95.56441626137298, Reward std: 120.86291625111215, Alpha: 0.18868482112884521\n",
      "Step: 345001, Reward mean: 215.59196730755247, Reward std: 32.510983102077425, Alpha: 0.18926939368247986\n",
      "Step: 350001, Reward mean: 166.61629188324622, Reward std: 88.23032792774427, Alpha: 0.18984319269657135\n",
      "Step: 355001, Reward mean: 210.06192467687484, Reward std: 33.65093242520125, Alpha: 0.1897488534450531\n",
      "Step: 360001, Reward mean: 177.66578662257612, Reward std: 61.081479669199155, Alpha: 0.19033488631248474\n",
      "Step: 365001, Reward mean: 147.71278232954876, Reward std: 102.7158624530945, Alpha: 0.19323378801345825\n",
      "Step: 370001, Reward mean: 194.93058533277087, Reward std: 49.58022610918845, Alpha: 0.19365470111370087\n",
      "Step: 375001, Reward mean: 210.99018117194458, Reward std: 33.966639677526786, Alpha: 0.1944175362586975\n",
      "Step: 380001, Reward mean: 209.3151183536686, Reward std: 28.921784189186987, Alpha: 0.19749964773654938\n",
      "Step: 385001, Reward mean: 148.45952153845556, Reward std: 85.48982256340747, Alpha: 0.20067068934440613\n",
      "Step: 390001, Reward mean: 193.41802200392655, Reward std: 54.67327084300782, Alpha: 0.20190541446208954\n",
      "Step: 395001, Reward mean: 136.54563714422574, Reward std: 86.95371730766993, Alpha: 0.20397473871707916\n",
      "Step: 400001, Reward mean: 182.17913222999493, Reward std: 59.25030492447276, Alpha: 0.20419612526893616\n",
      "Step: 405001, Reward mean: 167.70999298085297, Reward std: 97.2471488593461, Alpha: 0.20723719894886017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 410001, Reward mean: 179.21532990048104, Reward std: 81.82498783850546, Alpha: 0.20782804489135742\n",
      "Step: 415001, Reward mean: 127.37628775506582, Reward std: 101.07372094012075, Alpha: 0.20849010348320007\n",
      "Step: 420001, Reward mean: 167.2306707739204, Reward std: 103.1975844256442, Alpha: 0.20752054452896118\n",
      "Step: 425001, Reward mean: 115.64750333224443, Reward std: 100.91810013838631, Alpha: 0.20657847821712494\n",
      "Step: 430001, Reward mean: 94.16646484858792, Reward std: 110.00212085965082, Alpha: 0.20775116980075836\n",
      "Step: 435001, Reward mean: 158.2136428087138, Reward std: 91.28451668858347, Alpha: 0.20719581842422485\n",
      "Step: 440001, Reward mean: 48.581487965619715, Reward std: 119.5568279782198, Alpha: 0.20697815716266632\n",
      "Step: 445001, Reward mean: 137.59563185768798, Reward std: 86.91682977845039, Alpha: 0.20851373672485352\n",
      "Step: 450001, Reward mean: 147.41157018239574, Reward std: 106.82272611396827, Alpha: 0.20984292030334473\n",
      "Step: 455001, Reward mean: 138.11774729979345, Reward std: 128.57823851372692, Alpha: 0.21125024557113647\n",
      "Step: 460001, Reward mean: 166.30643946265485, Reward std: 112.19869270122993, Alpha: 0.21227383613586426\n",
      "Step: 465001, Reward mean: 97.76919809757155, Reward std: 98.0494493133464, Alpha: 0.210765540599823\n",
      "Step: 470001, Reward mean: 72.00656301459892, Reward std: 122.1374592267733, Alpha: 0.20855167508125305\n",
      "Step: 475001, Reward mean: 163.70526279231075, Reward std: 107.51985767357384, Alpha: 0.20681160688400269\n",
      "Step: 480001, Reward mean: 72.79003310914597, Reward std: 148.52857673708453, Alpha: 0.20484578609466553\n",
      "Step: 485001, Reward mean: 121.23632365758331, Reward std: 150.58878335125007, Alpha: 0.20223738253116608\n",
      "Step: 490001, Reward mean: 35.952229379779375, Reward std: 135.5868144206707, Alpha: 0.20026323199272156\n",
      "Step: 495001, Reward mean: 68.54606576513837, Reward std: 139.60188646954694, Alpha: 0.1994556188583374\n",
      "Step: 500000, Reward mean: 159.76478877617814, Reward std: 130.79301901242982, Alpha: 0.19681772589683533\n"
     ]
    }
   ],
   "source": [
    "config1 = {\n",
    "    \"agent\": {\n",
    "        \"state_size\": 8,\n",
    "        \"action_size\": 4,\n",
    "        \"hidden_size\": 32,\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.001,\n",
    "        \"target_entropy_scale\": 0.5, # 0.5\n",
    "        \"actor_lr\": 2e-4,\n",
    "        \"critic_lr\": 5e-4,\n",
    "        \"alpha_lr\": 1e-5 # 1e-5\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"seed\": 0,\n",
    "        \"timesteps\": 500_000,\n",
    "        \"start_train\": 10_000,\n",
    "        \"buffer_size\": 200_000, # better than int(1e6)\n",
    "        \"batch_size\": 128,\n",
    "        \"test_episodes\": 10,\n",
    "        \"test_every\": 5_000,\n",
    "        \"update_every\": 16 # 16\n",
    "    }\n",
    "}\n",
    "\n",
    "model1 = SoftActorCritic(**config1[\"agent\"])\n",
    "mean1, std1 = train(\"LunarLander-v2\", model1, **config1[\"trainer\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SAC_DIS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
